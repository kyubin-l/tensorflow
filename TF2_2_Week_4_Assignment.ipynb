{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "coursera": {
      "course_slug": "tensor-flow-2-2",
      "graded_item_id": "2x3vn",
      "launcher_item_id": "QKXZc"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "TF2_2 Week 4 Assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kyubin-l/tensorflow/blob/main/TF2_2_Week_4_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTehttYqbiZO"
      },
      "source": [
        "# Programming Assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0d-2RopbiZQ"
      },
      "source": [
        "## Residual network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yK03DPBRbiZR"
      },
      "source": [
        "### Instructions\n",
        "\n",
        "In this notebook, you will use the model subclassing API together with custom layers to create a residual network architecture. You will then train your custom model on the Fashion-MNIST dataset by using a custom training loop and implementing the automatic differentiation tools in Tensorflow to calculate the gradients for backpropagation.\n",
        "\n",
        "Some code cells are provided you in the notebook. You should avoid editing provided code, and make sure to execute the cells in order to avoid unexpected errors. Some cells begin with the line: \n",
        "\n",
        "`#### GRADED CELL ####`\n",
        "\n",
        "Don't move or edit this first line - this is what the automatic grader looks for to recognise graded cells. These cells require you to write your own code to complete them, and are automatically graded when you submit the notebook. Don't edit the function name or signature provided in these cells, otherwise the automatic grader might not function properly. Inside these graded cells, you can use any functions or classes that are imported below, but make sure you don't use any variables that are outside the scope of the function.\n",
        "\n",
        "### How to submit\n",
        "\n",
        "Complete all the tasks you are asked for in the worksheet. When you have finished and are happy with your code, press the **Submit Assignment** button at the top of this notebook.\n",
        "\n",
        "### Let's get started!\n",
        "\n",
        "We'll start running some imports, and loading the dataset. Do not edit the existing imports in the following cell. If you would like to make further Tensorflow imports, you should add them here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2HDFAM9JbiZT"
      },
      "source": [
        "#### PACKAGE IMPORTS ####\n",
        "\n",
        "# Run this cell first to import all required packages. Do not make any imports elsewhere in the notebook\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Layer, BatchNormalization, Conv2D, Dense, Flatten, Add\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# If you would like to make further imports from tensorflow, add them here\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LURF_ZYbiZX"
      },
      "source": [
        "#### The Fashion-MNIST dataset\n",
        "\n",
        "In this assignment, you will use the [Fashion-MNIST dataset](https://github.com/zalandoresearch/fashion-mnist). It consists of a training set of 60,000 images of fashion items with corresponding labels, and a test set of 10,000 images. The images have been normalised and centred. The dataset is frequently used in machine learning research, especially as a drop-in replacement for the MNIST dataset. \n",
        "\n",
        "- H. Xiao, K. Rasul, and R. Vollgraf. \"Fashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning Algorithms.\" arXiv:1708.07747, August 2017.\n",
        "\n",
        "Your goal is to construct a ResNet model that classifies images of fashion items into one of 10 classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JikW1jd8biZY"
      },
      "source": [
        "#### Load the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38Z9Eoq4biZZ"
      },
      "source": [
        "For this programming assignment, we will take a smaller sample of the dataset to reduce the training time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ur9hxTa4biZa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee37696e-18a1-48ba-ba1d-9b29e6c16f3f"
      },
      "source": [
        "# Load and preprocess the Fashion-MNIST dataset\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()\n",
        "\n",
        "train_images = train_images.astype(np.float32)\n",
        "test_images = test_images.astype(np.float32)\n",
        "\n",
        "train_images = train_images[:5000] / 255.\n",
        "train_labels = train_labels[:5000]\n",
        "\n",
        "test_images = test_images / 255.\n",
        "\n",
        "train_images = train_images[..., np.newaxis]\n",
        "test_images = test_images[..., np.newaxis]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_nl7FV8GbiZe"
      },
      "source": [
        "# Create Dataset objects for the training and test sets\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_images, train_labels))\n",
        "train_dataset = train_dataset.batch(32)\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_images, test_labels))\n",
        "test_dataset = test_dataset.batch(32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VsLCYrPbiZh"
      },
      "source": [
        "# Get dataset labels\n",
        "\n",
        "image_labels = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UKYy3Q1biZk"
      },
      "source": [
        "#### Create custom layers for the residual blocks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqm8M28ZbiZl"
      },
      "source": [
        "You should now create a first custom layer for a residual block of your network. Using layer subclassing, build your custom layer according to the following spec:\n",
        "\n",
        "* The custom layer class should have `__init__`, `build` and `call` methods. The `__init__` method has been completed for you. It calls the base `Layer` class initializer, passing on any keyword arguments\n",
        "* The `build` method should create the layers. It will take an `input_shape` argument, and should extract the number of filters from this argument. It should create:\n",
        "    * A BatchNormalization layer: this will be the first layer in the block, so should use its `input shape` keyword argument\n",
        "    * A Conv2D layer with the same number of filters as the layer input, a 3x3 kernel size, `'SAME'` padding, and no activation function\n",
        "    * Another BatchNormalization layer\n",
        "    * Another Conv2D layer, again with the same number of filters as the layer input, a 3x3 kernel size, `'SAME'` padding, and no activation function\n",
        "* The `call` method should then process the input through the layers:\n",
        "    * The first BatchNormalization layer: ensure to set the `training` keyword argument\n",
        "    * A `tf.nn.relu` activation function\n",
        "    * The first Conv2D layer\n",
        "    * The second BatchNormalization layer: ensure to set the `training` keyword argument\n",
        "    * Another `tf.nn.relu` activation function\n",
        "    * The second Conv2D layer\n",
        "    * It should then add the layer inputs to the output of the second Conv2D layer. This is the final layer output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mC1oYU0QSWoJ"
      },
      "source": [
        "from tensorflow.keras.layers import BatchNormalization, Conv2D, Flatten"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWXDT-jWbiZm"
      },
      "source": [
        "#### GRADED CELL ####\n",
        "\n",
        "# Complete the following class. \n",
        "# Make sure to not change the class or method names or arguments.\n",
        "\n",
        "class ResidualBlock(Layer):\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(ResidualBlock, self).__init__(**kwargs)\n",
        "        \n",
        "    def build(self, input_shape):\n",
        "        \"\"\"\n",
        "        This method should build the layers according to the above specification. Make sure \n",
        "        to use the input_shape argument to get the correct number of filters, and to set the\n",
        "        input_shape of the first layer in the block.\n",
        "        \"\"\"\n",
        "\n",
        "        num_filters = input_shape[-1]\n",
        "        self.batch_normalization_1 = BatchNormalization(input_shape=input_shape)\n",
        "        self.conv2d_1 = Conv2D(num_filters, (3,3), padding='same')\n",
        "        self.batch_normalization_2 = BatchNormalization()\n",
        "        self.conv2d_2 = Conv2D(num_filters, (3,3), padding='same')\n",
        "        \n",
        "        \n",
        "    def call(self, inputs, training=False):\n",
        "        \"\"\"\n",
        "        This method should contain the code for calling the layer according to the above\n",
        "        specification, using the layer objects set up in the build method.\n",
        "        \"\"\"\n",
        "        input_layer = self.batch_normalization_1(inputs, training=training)\n",
        "        h = tf.nn.relu(input_layer)\n",
        "        h = self.conv2d_1(input_layer)\n",
        "        h = self.batch_normalization_2(h, training=training)\n",
        "        h = tf.nn.relu(h)\n",
        "        h = self.conv2d_2(h)\n",
        "        h = tf.add(inputs, h)\n",
        "        return h\n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SF8Goq8LbiZo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "127f77fd-8266-4b5b-f33f-c6c3cd78836b"
      },
      "source": [
        "# Test your custom layer - the following should create a model using your layer\n",
        "\n",
        "test_model = tf.keras.Sequential([ResidualBlock(input_shape=(28, 28, 1), name=\"residual_block\")])\n",
        "test_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "residual_block (ResidualBloc (None, 28, 28, 1)         28        \n",
            "=================================================================\n",
            "Total params: 28\n",
            "Trainable params: 24\n",
            "Non-trainable params: 4\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2PoqmxCbiZy"
      },
      "source": [
        "You should now create a second custom layer for a residual block of your network. This layer will be used to change the number of filters within the block. Using layer subclassing, build your custom layer according to the following spec:\n",
        "\n",
        "* The custom layer class should have `__init__`, `build` and `call` methods \n",
        "* The class initialiser should call the base `Layer` class initializer, passing on any keyword arguments. It should also accept a `out_filters` argument, and save it as a class attribute\n",
        "* The `build` method should create the layers. It will take an `input_shape` argument, and should extract the number of input filters from this argument. It should create:\n",
        "    * A BatchNormalization layer: this will be the first layer in the block, so should use its `input shape` keyword argument\n",
        "    * A Conv2D layer with the same number of filters as the layer input, a 3x3 kernel size, `\"SAME\"` padding, and no activation function\n",
        "    * Another BatchNormalization layer\n",
        "    * Another Conv2D layer with `out_filters` number of filters, a 3x3 kernel size, `\"SAME\"` padding, and no activation function\n",
        "    * A final Conv2D layer with `out_filters` number of filters, a 1x1 kernel size, and no activation function\n",
        "* The `call` method should then process the input through the layers:\n",
        "    * The first BatchNormalization layer: ensure to set the `training` keyword argument\n",
        "    * A `tf.nn.relu` activation function\n",
        "    * The first Conv2D layer\n",
        "    * The second BatchNormalization layer: ensure to set the `training` keyword argument\n",
        "    * Another `tf.nn.relu` activation function\n",
        "    * The second Conv2D layer\n",
        "    * It should then take the layer inputs, pass it through the final 1x1 Conv2D layer, and add to the output of the second Conv2D layer. This is the final layer output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HiK_lavabiZz"
      },
      "source": [
        "#### GRADED CELL ####\n",
        "\n",
        "# Complete the following class. \n",
        "# Make sure to not change the class or method names or arguments.\n",
        "\n",
        "class FiltersChangeResidualBlock(Layer):\n",
        "\n",
        "    def __init__(self, out_filters, **kwargs):\n",
        "        \"\"\"\n",
        "        The class initialiser should call the base class initialiser, passing any keyword\n",
        "        arguments along. It should also set the number of filters as a class attribute.\n",
        "        \"\"\"\n",
        "        super(FiltersChangeResidualBlock, self).__init__(**kwargs)\n",
        "        self.out_filters = out_filters\n",
        "\n",
        "             \n",
        "    def build(self, input_shape):\n",
        "        \"\"\"\n",
        "        This method should build the layers according to the above specification. Make sure \n",
        "        to use the input_shape argument to get the correct number of filters, and to set the\n",
        "        input_shape of the first layer in the block.\n",
        "        \"\"\"\n",
        "        num_filters = input_shape[-1]\n",
        "        self.batch_normalization_1 = BatchNormalization(input_shape=input_shape)\n",
        "        self.conv2d_1 = Conv2D(self.out_filters, (3,3), padding='same')\n",
        "        self.batch_normalization_2 = BatchNormalization()\n",
        "        self.conv2d_2 = Conv2D(self.out_filters, (3,3), padding='same')\n",
        "        self.conv2d_3 = Conv2D(self.out_filters, (1,1))       \n",
        "        \n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        \"\"\"\n",
        "        This method should contain the code for calling the layer according to the above\n",
        "        specification, using the layer objects set up in the build method.\n",
        "        \"\"\"\n",
        "        input_layer = self.batch_normalization_1(inputs, training=training)\n",
        "        x = tf.nn.relu(input_layer)\n",
        "        x = self.conv2d_1(x)\n",
        "        x = self.batch_normalization_2(x, training=training)\n",
        "        x = tf.nn.relu(x)\n",
        "        x = self.conv2d_2(x)\n",
        "        x_2 = self.conv2d_3(inputs)\n",
        "        return tf.add(x, x_2)\n",
        "\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWacV9I5biZ3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7104f73f-f4d0-4388-e9ff-18b14cda9730"
      },
      "source": [
        "# Test your custom layer - the following should create a model using your layer\n",
        "\n",
        "test_model = tf.keras.Sequential([FiltersChangeResidualBlock(16, input_shape=(32, 32, 3), name=\"fc_resnet_block\")])\n",
        "test_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_15\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "fc_resnet_block (FiltersChan (None, 32, 32, 16)        2908      \n",
            "=================================================================\n",
            "Total params: 2,908\n",
            "Trainable params: 2,870\n",
            "Non-trainable params: 38\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMY3Ak7YbiZ6"
      },
      "source": [
        "#### Create a custom model that integrates the residual blocks\n",
        "\n",
        "You are now ready to build your ResNet model. Using model subclassing, build your model according to the following spec:\n",
        "\n",
        "* The custom model class should have `__init__` and `call` methods. \n",
        "* The class initialiser should call the base `Model` class initializer, passing on any keyword arguments. It should create the model layers:\n",
        "    * The first Conv2D layer, with 32 filters, a 7x7 kernel and stride of 2.\n",
        "    * A `ResidualBlock` layer.\n",
        "    * The second Conv2D layer, with 32 filters, a 3x3 kernel and stride of 2.\n",
        "    * A `FiltersChangeResidualBlock` layer, with 64 output filters.\n",
        "    * A Flatten layer\n",
        "    * A final Dense layer, with a 10-way softmax output\n",
        "* The `call` method should then process the input through the layers in the order given above. Ensure to pass the `training` keyword argument to the residual blocks, to ensure the correct mode of operation for the batch norm layers.\n",
        "\n",
        "In total, your neural network should have six layers (counting each residual block as one layer)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53xs9JBKbiZ7"
      },
      "source": [
        "#### GRADED CELL ####\n",
        "\n",
        "# Complete the following class. \n",
        "# Make sure to not change the class or method names or arguments.\n",
        "\n",
        "class ResNetModel(Model):\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        \"\"\"\n",
        "        The class initialiser should call the base class initialiser, passing any keyword\n",
        "        arguments along. It should also create the layers of the network according to the\n",
        "        above specification.\n",
        "        \"\"\"\n",
        "        super(ResNetModel, self).__init__(**kwargs)\n",
        "        self.conv2D_1 = Conv2D(32, (7, 7), strides=2)\n",
        "        self.resblock = ResidualBlock()\n",
        "        self.conv2D_2 = Conv2D(32, (3,3), strides=2)\n",
        "        self.filter_change_resblock = FiltersChangeResidualBlock(64)\n",
        "        self.flatten = Flatten()\n",
        "        self.dense = Dense(10, activation='softmax')\n",
        "\n",
        "        \n",
        "    def call(self, inputs, training=False):\n",
        "        \"\"\"\n",
        "        This method should contain the code for calling the layer according to the above\n",
        "        specification, using the layer objects set up in the initialiser.\n",
        "        \"\"\"\n",
        "        \n",
        "        x = self.conv2D_1(inputs)\n",
        "        x = self.resblock(x, training=training)\n",
        "        x = self.conv2D_2(x)\n",
        "        x = self.filter_change_resblock(x, training=training)\n",
        "        x = self.flatten(x)\n",
        "        x = self.dense(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZG77KapbiZ-"
      },
      "source": [
        "# Create the model\n",
        "\n",
        "resnet_model = ResNetModel()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlHXCYYLbiaB"
      },
      "source": [
        "#### Define the optimizer and loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxfc-oYdbiaB"
      },
      "source": [
        "We will use the Adam optimizer with a learning rate of 0.001, and the sparse categorical cross entropy function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C33dTTFzbiaC"
      },
      "source": [
        "# Create the optimizer and loss\n",
        "\n",
        "optimizer_obj = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "loss_obj = tf.keras.losses.SparseCategoricalCrossentropy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAhzpm3TbiaG"
      },
      "source": [
        "#### Define the grad function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFL3r1zZbiaH"
      },
      "source": [
        "You should now create the `grad` function that will compute the forward and backward pass, and return the loss value and gradients that will be used in your custom training loop:\n",
        "\n",
        "* The `grad` function takes a model instance, inputs, targets and the loss object above as arguments\n",
        "* The function should use a `tf.GradientTape` context to compute the forward pass and calculate the loss\n",
        "* The function should compute the gradient of the loss with respect to the model's trainable variables\n",
        "* The function should return a tuple of two elements: the loss value, and a list of gradients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6k5MsJVbiaI"
      },
      "source": [
        "#### GRADED CELL ####\n",
        "\n",
        "# Complete the following function. \n",
        "# Make sure to not change the function name or arguments.\n",
        "\n",
        "@tf.function\n",
        "def grad(model, inputs, targets, loss):\n",
        "    \"\"\"\n",
        "    This function should compute the loss and gradients of your model, corresponding to\n",
        "    the inputs and targets provided. It should return the loss and gradients.\n",
        "    \"\"\"\n",
        "    with tf.GradientTape() as tape:\n",
        "        loss_value = loss(model, inputs, targets)\n",
        "\n",
        "    return loss_value, tape.gradient(loss_value, model.trainable_variables)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_48RRAGbiaK"
      },
      "source": [
        "#### Define the custom training loop"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDKxVoDEbiaL"
      },
      "source": [
        "You should now write a custom training loop. Complete the following function, according to the spec:\n",
        "\n",
        "* The function takes the following arguments:\n",
        "    * `model`: an instance of your custom model\n",
        "    * `num_epochs`: integer number of epochs to train the model\n",
        "    * `dataset`: a `tf.data.Dataset` object for the training data\n",
        "    * `optimizer`: an optimizer object, as created above\n",
        "    * `loss`: a sparse categorical cross entropy object, as created above\n",
        "    * `grad_fn`: your `grad` function above, that returns the loss and gradients for given model, inputs and targets\n",
        "* Your function should train the model for the given number of epochs, using the `grad_fn` to compute gradients for each training batch, and updating the model parameters using `optimizer.apply_gradients`. \n",
        "* Your function should collect the mean loss and accuracy values over the epoch, and return a tuple of two lists; the first for the list of loss values per epoch, the second for the list of accuracy values per epoch.\n",
        "\n",
        "You may also want to print out the loss and accuracy at each epoch during the training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVcLmdduj7eD"
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOvFcrLpbiaM"
      },
      "source": [
        "#### GRADED CELL ####\n",
        "\n",
        "# Complete the following function. \n",
        "# Make sure to not change the function name or arguments.\n",
        "\n",
        "def train_resnet(model, num_epochs, dataset, optimizer, loss, grad_fn):\n",
        "    \"\"\"\n",
        "    This function should implement the custom training loop, as described above. It should \n",
        "    return a tuple of two elements: the first element is a list of loss values per epoch, the\n",
        "    second is a list of accuracy values per epoch\n",
        "    \"\"\"\n",
        "    \n",
        "    train_loss_results = []\n",
        "    train_accuracy_results = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss_avg = tf.keras.metrics.Mean()\n",
        "        epoch_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
        "\n",
        "        for x, y in dataset:\n",
        "            loss_value, grads = grad_fn(model, x, y, loss)\n",
        "            optmizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "            epoch_loss_avg(loss_value)\n",
        "            train_accuracy_results(to_categorical(y), model(x))\n",
        "\n",
        "        train_loss_results.append(epoch_loss_avg.result())\n",
        "        train_accuracy_results.append(epoch_accuracy.result())\n",
        "\n",
        "        print(f'Epoch: {epoch:03d}, Loss: {epoch_loss_avg.result():.3f}, Accuracy: {epoch_accuracy.result():.3%}')\n",
        "\n",
        "    return (train_loss_results, train_accuracy_results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yD9Pxs_PbiaO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "outputId": "1bcb7629-bc6c-48f9-d3a5-80c1f410f411"
      },
      "source": [
        "# Train the model for 8 epochs\n",
        "\n",
        "train_loss_results, train_accuracy_results = train_resnet(resnet_model, \n",
        "                                                          8, \n",
        "                                                          train_dataset, \n",
        "                                                          optimizer_obj, \n",
        "                                                          loss_obj, \n",
        "                                                          grad)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-95-e6c924f8c071>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m                                                           \u001b[0moptimizer_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                                                           \u001b[0mloss_obj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                                                           grad)\n\u001b[0m",
            "\u001b[0;32m<ipython-input-92-f58641b1ad89>\u001b[0m in \u001b[0;36mtrain_resnet\u001b[0;34m(model, num_epochs, dataset, optimizer, loss, grad_fn)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0moptmizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    887\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    922\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 924\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3020\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3021\u001b[0m       (graph_function,\n\u001b[0;32m-> 3022\u001b[0;31m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0m\u001b[1;32m   3023\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m   3024\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3443\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3444\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3445\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3287\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3288\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3289\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3290\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    997\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 999\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    670\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    984\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    985\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 986\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    987\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    <ipython-input-48-2507daf7e3ef>:13 grad  *\n        loss_value = loss(model, inputs, targets)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/losses.py:155 __call__  **\n        losses = call_fn(y_true, y_pred)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/losses.py:259 call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/losses.py:1711 sparse_categorical_crossentropy\n        y_true = math_ops.cast(y_true, y_pred.dtype)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py:206 wrapper\n        return target(*args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/math_ops.py:988 cast\n        x = ops.convert_to_tensor(x, name=\"x\")\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/profiler/trace.py:163 wrapped\n        return func(*args, **kwargs)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py:1566 convert_to_tensor\n        ret = conversion_func(value, dtype=dtype, name=name, as_ref=as_ref)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py:339 _constant_tensor_conversion_function\n        return constant(v, dtype=dtype, name=name)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py:265 constant\n        allow_broadcast=True)\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/constant_op.py:283 _constant_impl\n        allow_broadcast=allow_broadcast))\n    /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/tensor_util.py:553 make_tensor_proto\n        \"supported type.\" % (type(values), values))\n\n    TypeError: Failed to convert object of type <class '__main__.ResNetModel'> to Tensor. Contents: <__main__.ResNetModel object at 0x7f709cc3cf10>. Consider casting elements to a supported type.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9w0AOlo2biaR"
      },
      "source": [
        "#### Plot the learning curves"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQxQxQxgbiaS"
      },
      "source": [
        "fig, axes = plt.subplots(1, 2, sharex=True, figsize=(12, 5))\n",
        "\n",
        "axes[0].set_xlabel(\"Epochs\", fontsize=14)\n",
        "axes[0].set_ylabel(\"Loss\", fontsize=14)\n",
        "axes[0].set_title('Loss vs epochs')\n",
        "axes[0].plot(train_loss_results)\n",
        "\n",
        "axes[1].set_title('Accuracy vs epochs')\n",
        "axes[1].set_ylabel(\"Accuracy\", fontsize=14)\n",
        "axes[1].set_xlabel(\"Epochs\", fontsize=14)\n",
        "axes[1].plot(train_accuracy_results)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQ_A0So1biaU"
      },
      "source": [
        "#### Evaluate the model performance on the test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVyiUCMjbiaV"
      },
      "source": [
        "# Compute the test loss and accuracy\n",
        "\n",
        "epoch_loss_avg = tf.keras.metrics.Mean()\n",
        "epoch_accuracy = tf.keras.metrics.CategoricalAccuracy()\n",
        "\n",
        "for x, y in test_dataset:\n",
        "    model_output = resnet_model(x)\n",
        "    epoch_loss_avg(loss_obj(y, model_output))  \n",
        "    epoch_accuracy(to_categorical(y), model_output)\n",
        "\n",
        "print(\"Test loss: {:.3f}\".format(epoch_loss_avg.result().numpy()))\n",
        "print(\"Test accuracy: {:.3%}\".format(epoch_accuracy.result().numpy()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTV7htZDbiaX"
      },
      "source": [
        "#### Model predictions\n",
        "\n",
        "Let's see some model predictions! We will randomly select four images from the test data, and display the image and label for each. \n",
        "\n",
        "For each test image, model's prediction (the label with maximum probability) is shown, together with a plot showing the model's categorical distribution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1o1RDRWEbiaY"
      },
      "source": [
        "# Run this cell to get model predictions on randomly selected test images\n",
        "\n",
        "num_test_images = test_images.shape[0]\n",
        "\n",
        "random_inx = np.random.choice(test_images.shape[0], 4)\n",
        "random_test_images = test_images[random_inx, ...]\n",
        "random_test_labels = test_labels[random_inx, ...]\n",
        "\n",
        "predictions = resnet_model(random_test_images)\n",
        "\n",
        "fig, axes = plt.subplots(4, 2, figsize=(16, 12))\n",
        "fig.subplots_adjust(hspace=0.5, wspace=-0.2)\n",
        "\n",
        "for i, (prediction, image, label) in enumerate(zip(predictions, random_test_images, random_test_labels)):\n",
        "    axes[i, 0].imshow(np.squeeze(image))\n",
        "    axes[i, 0].get_xaxis().set_visible(False)\n",
        "    axes[i, 0].get_yaxis().set_visible(False)\n",
        "    axes[i, 0].text(5., -2., f'Class {label} ({image_labels[label]})')\n",
        "    axes[i, 1].bar(np.arange(len(prediction)), prediction)\n",
        "    axes[i, 1].set_xticks(np.arange(len(prediction)))\n",
        "    axes[i, 1].set_xticklabels(image_labels, rotation=0)\n",
        "    pred_inx = np.argmax(prediction)\n",
        "    axes[i, 1].set_title(f\"Categorical distribution. Model prediction: {image_labels[pred_inx]}\")\n",
        "    \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S6qxLOmbbiaa"
      },
      "source": [
        "Congratulations for completing this programming assignment! You're now ready to move on to the capstone project for this course."
      ]
    }
  ]
}